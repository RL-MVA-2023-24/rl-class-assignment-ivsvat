{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Training notebook for Reinforcement Learning project submission.\n",
    "\n",
    "By Ivan SVATKO, MVA 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from joblib import dump, load\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from env_hiv import HIVPatient\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things that worked\n",
    "\n",
    "Below we implement a version of Fitted Q iteration inspired by Ernst et al. 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "120 patiens -> 120*200 = 24000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_samples(env, total_iter, disable_tqdm=False, print_done_states=False, Q_hat=None, eps=0.15):\n",
    "    \"\"\"\n",
    "        samples trajectories from the environment\n",
    "        if provided, uses an estimated Q function with eps-greedy policy\n",
    "        otherwise picks a random action\n",
    "\n",
    "        Adapted from the sampler used in the class\n",
    "    \"\"\"\n",
    "    s, _ = env.reset()\n",
    "    #dataset = []\n",
    "    S = []\n",
    "    A = []\n",
    "    R = []\n",
    "    current_R = []\n",
    "    S2 = []\n",
    "    D = []\n",
    "    it = 0\n",
    "    cumulative_rewards = []\n",
    "    for _ in tqdm(range(total_iter), disable=disable_tqdm):\n",
    "        if Q_hat is None:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            u = np.random.rand(1)\n",
    "            if u>eps:\n",
    "                Qsa =[]\n",
    "                for a in range(env.action_space.n):\n",
    "                    sa = np.append(s,a).reshape(1, -1)\n",
    "                    Qsa.append(Q_hat.predict(sa))\n",
    "                a = np.argmax(Qsa)\n",
    "            else:\n",
    "                a = env.action_space.sample()\n",
    "        s2, r, done, trunc, _ = env.step(a)\n",
    "\n",
    "        #dataset.append((s,a,r,s2,done,trunc))\n",
    "        S.append(s)\n",
    "        A.append(a)\n",
    "        R.append(r)\n",
    "        S2.append(s2)\n",
    "        current_R.append(r)\n",
    "        D.append(done)\n",
    "        it += 1\n",
    "        if done or trunc:\n",
    "            cumulative_rewards.append(np.sum(current_R))\n",
    "            s, _ = env.reset()\n",
    "            current_R = []\n",
    "            if done and print_done_states:\n",
    "                print(\"done!\")\n",
    "        else:\n",
    "            s = s2\n",
    "    S = np.array(S)\n",
    "    A = np.array(A).reshape((-1,1))\n",
    "    R = np.array(R)\n",
    "    S2= np.array(S2)\n",
    "    D = np.array(D)\n",
    "    return S, A, R, S2, D, np.mean(cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def et_fqi(S, A, R, S2, D, iterations, nb_actions, gamma, Q_start = None):\n",
    "    \"\"\"\n",
    "        Adapted from the FQI loop from the class\n",
    "    \"\"\"\n",
    "    nb_samples = S.shape[0]\n",
    "    Qfunctions = Q_start\n",
    "    SA = np.append(S,A,axis=1)\n",
    "    for it in tqdm(range(iterations)):\n",
    "        if Qfunctions is None:\n",
    "             value=R.copy()\n",
    "        else:\n",
    "            Q2 = np.zeros((nb_samples,nb_actions))\n",
    "            for a2 in range(nb_actions):\n",
    "                A2 = a2*np.ones((S.shape[0],1))\n",
    "                S2A2 = np.append(S2,A2,axis=1)\n",
    "                Q2[:,a2] = Qfunctions.predict(S2A2)\n",
    "            max_Q2 = np.max(Q2,axis=1)\n",
    "            value = R + gamma*(1-D)*max_Q2\n",
    "        Q = HistGradientBoostingRegressor()\n",
    "        Q.fit(SA,value)\n",
    "        Qfunctions = Q\n",
    "    return Qfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stages loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting the first sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [02:38<00:00, 25.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 0 \t strategy: None_0 \t reward: 1.041681e+07\n",
      "Fitting the Q function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:40<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling with: \t HistGradientBoostingRegressor()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [03:10<00:00, 20.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: 1 \t strategy: HistGradientBoostingRegressor()_1 \t reward: 4.904563e+09\n",
      "Fitting the Q function, sample size: \t 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:02<00:45,  2.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 65\u001b[0m\n\u001b[0;32m     61\u001b[0m _s \u001b[38;5;241m=\u001b[39m dump(sample, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msample_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;241m+\u001b[39mlag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting the Q function, sample size: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mS\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m Q_next \u001b[38;5;241m=\u001b[39m \u001b[43met_fqi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfqi_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQ_functions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m _s \u001b[38;5;241m=\u001b[39m dump(Q_next, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;241m+\u001b[39mlag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m Q_functions\u001b[38;5;241m.\u001b[39mappend(Q_next)\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36met_fqi\u001b[1;34m(S, A, R, S2, D, iterations, nb_actions, gamma, Q_start)\u001b[0m\n\u001b[0;32m     18\u001b[0m         value \u001b[38;5;241m=\u001b[39m R \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mD)\u001b[38;5;241m*\u001b[39mmax_Q2\n\u001b[0;32m     19\u001b[0m     Q \u001b[38;5;241m=\u001b[39m HistGradientBoostingRegressor()\n\u001b[1;32m---> 20\u001b[0m     \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     Qfunctions \u001b[38;5;241m=\u001b[39m Q\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Qfunctions\n",
      "File \u001b[1;32mc:\\Users\\ivans\\Dev\\Python_general\\Python_datascience\\.venv\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivans\\Dev\\Python_general\\Python_datascience\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:696\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trees_per_iteration_):\n\u001b[0;32m    679\u001b[0m     grower \u001b[38;5;241m=\u001b[39m TreeGrower(\n\u001b[0;32m    680\u001b[0m         X_binned\u001b[38;5;241m=\u001b[39mX_binned_train,\n\u001b[0;32m    681\u001b[0m         gradients\u001b[38;5;241m=\u001b[39mg_view[:, k],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    694\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    695\u001b[0m     )\n\u001b[1;32m--> 696\u001b[0m     \u001b[43mgrower\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m     acc_apply_split_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grower\u001b[38;5;241m.\u001b[39mtotal_apply_split_time\n\u001b[0;32m    699\u001b[0m     acc_find_split_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grower\u001b[38;5;241m.\u001b[39mtotal_find_split_time\n",
      "File \u001b[1;32mc:\\Users\\ivans\\Dev\\Python_general\\Python_datascience\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py:366\u001b[0m, in \u001b[0;36mTreeGrower.grow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Grow the tree, from root to leaves.\"\"\"\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplittable_nodes:\n\u001b[1;32m--> 366\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_shrinkage()\n",
      "File \u001b[1;32mc:\\Users\\ivans\\Dev\\Python_general\\Python_datascience\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py:586\u001b[0m, in \u001b[0;36mTreeGrower.split_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;66;03m# We use the brute O(n_samples) method on the child that has the\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;66;03m# smallest number of samples, and the subtraction trick O(n_bins)\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;66;03m# on the other one.\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# Note that both left and right child have the same allowed_features.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m tic \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 586\u001b[0m smallest_child\u001b[38;5;241m.\u001b[39mhistograms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_histograms_brute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmallest_child\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmallest_child\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallowed_features\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m largest_child\u001b[38;5;241m.\u001b[39mhistograms \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistogram_builder\u001b[38;5;241m.\u001b[39mcompute_histograms_subtraction(\n\u001b[0;32m    591\u001b[0m         node\u001b[38;5;241m.\u001b[39mhistograms,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    594\u001b[0m     )\n\u001b[0;32m    595\u001b[0m )\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_compute_hist_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m tic\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_stages = 20\n",
    "n_patients = 20\n",
    "shuffle_stages = [3, 5, 7, 9, 11, 13, 15]\n",
    "Q_functions = [None]\n",
    "episode_length = 200\n",
    "fqi_iter = 100\n",
    "gamma = 0.98\n",
    "\n",
    "env = TimeLimit(\n",
    "             env=HIVPatient(domain_randomization=False), max_episode_steps=episode_length\n",
    "            )\n",
    "\n",
    "# -We either start from scratch or load a dataset an a model-\n",
    "# 0- stage\n",
    "\n",
    "print(\"Collecting the first sample\")\n",
    "S, A, R, S2, D, cum_rew = collect_samples(env, n_patients*episode_length, Q_hat=Q_functions[-1])\n",
    "sample = [S,A,R,S2,D]\n",
    "_s = dump(sample, f\"samples\\sample_{0}\")\n",
    "print(f\"Stage: {0} \\t strategy: {Q_functions[-1]}_{0} \\t reward: {cum_rew:e}\")\n",
    "print(\"Fitting the Q function\")\n",
    "\n",
    "Q_next = et_fqi(S, A, R, S2, D, fqi_iter, 4, gamma, Q_start = Q_functions[-1])\n",
    "Q_functions.append(Q_next)\n",
    "_s = dump(Q_next, f\"samples\\Q{0}\")\n",
    "\n",
    "\n",
    "# ------------------loading data and models------------------\n",
    "\n",
    "# use saved data\n",
    "#sample_saved = load(\"samples\\Run_20p_200it_100upd\\sample_19\")\n",
    "#S, A, R, S2, D = sample_saved\n",
    "\n",
    "# generate new data\n",
    "#S, A, R, S2, D, cum_rew = collect_samples(env, n_patients*episode_length, Q_hat=Q_functions[-1])\n",
    "\n",
    "#Q_functions.append(load(\"samples\\Run_20p_200it_100upd\\Q19\"))\n",
    "\n",
    "# number of pretrained epochs\n",
    "lag = 0\n",
    "\n",
    "for n in range(1, N_stages):\n",
    "    if n in shuffle_stages:\n",
    "        env = TimeLimit(\n",
    "             env=HIVPatient(domain_randomization=True), max_episode_steps=episode_length\n",
    "            )\n",
    "    else:\n",
    "        env = TimeLimit(\n",
    "             env=HIVPatient(domain_randomization=False), max_episode_steps=episode_length\n",
    "            )\n",
    "    print(f\"Sampling with: \\t {Q_functions[-1]}\")\n",
    "    S_next,A_next,R_next,S2_next,D_next, cum_rew = collect_samples(env, n_patients*episode_length, Q_hat=Q_functions[-1])\n",
    "    print(f\"Stage: {n+lag} \\t strategy: {Q_functions[-1]}_{n+lag} \\t reward: {cum_rew:e}\")\n",
    "    S = np.vstack([S, S_next])\n",
    "    A = np.vstack([A, A_next])\n",
    "    R = np.hstack([R, R_next])\n",
    "    S2 = np.vstack([S2, S2_next])\n",
    "    D = np.hstack([D, D_next])\n",
    "    sample = [S,A,R,S2,D]\n",
    "        \n",
    "    _s = dump(sample, f\"samples\\sample_{n+lag}\")\n",
    "\n",
    "    print(f\"Fitting the Q function, sample size: \\t {S.shape[0]}\")\n",
    "\n",
    "    Q_next = et_fqi(S, A, R, S2, D, fqi_iter, 4, gamma, Q_start = Q_functions[-1])\n",
    "    _s = dump(Q_next, f\"samples\\Q{n+lag}\")\n",
    "    Q_functions.append(Q_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things that didn't work\n",
    "\n",
    "Additionally, several versions of DQN were attempted. Notably target networks were explored. Using EMA weight update resulted in the most stable traing performace. The code below reuses the same code, modifying it to implement a clipped double DQN.\n",
    "\n",
    "These attempts were ultimately unsucsessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = TimeLimit(\n",
    "    env=HIVPatient(domain_randomization=False), max_episode_steps=200\n",
    ") \n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n \n",
    "nb_neurons=128\n",
    "DQN1 = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, n_action))\n",
    "\n",
    "DQN2 = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, n_action))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.data = []\n",
    "        self.index = 0 \n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = int((self.index + 1) % self.capacity)\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)), list(zip(*batch))))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "class ProjectAgent:\n",
    "    def __init__(self, config, model_1, model_2):\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.memory = ReplayBuffer(config['buffer_size'])\n",
    "        self.epsilon_max = config['epsilon_max']\n",
    "        self.epsilon_min = config['epsilon_min']\n",
    "        self.epsilon_stop = config['epsilon_decay_period']\n",
    "        self.epsilon_delay = config['epsilon_delay_decay']\n",
    "        self.warmup_steps = config['warmup_steps']\n",
    "        #self.update_target_strategy = config['update_target_strategy']# if 'update_target_strategy' in config.keys() else 'replace'\n",
    "        #self.update_target_freq = config['update_target_freq']# if 'update_target_freq' in config.keys() else 20\n",
    "        #self.update_target_tau = config['update_target_tau']# if 'update_target_tau' in config.keys() else 0.005\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.gamma = 0.98\n",
    "        self.model_1 = model_1 \n",
    "        self.model_2 = model_2\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer_1 = torch.optim.Adam(self.model_1.parameters(), lr=config['learning_rate'])\n",
    "        self.optimizer_2 = torch.optim.Adam(self.model_2.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.optimizer_1.zero_grad()\n",
    "            self.optimizer_2.zero_grad()\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            #QYmax = self.model(Y).max().detach().item()\n",
    "            QYmax_1, id = self.model_1(Y).max(1)\n",
    "       \n",
    "            QYmax_1 = QYmax_1.detach()\n",
    "\n",
    "            QYmax_2 = self.model_2(Y).detach()\n",
    "       \n",
    "            QYmax_2 = QYmax_2.gather(1, id.view(-1, 1)).detach()\n",
    "   \n",
    "            QYmin = torch.min(QYmax_1, QYmax_2)\n",
    "            #update = torch.addcmul(R, self.gamma, 1-D, QYmin).detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmin, value=self.gamma)\n",
    "            QXA_1 = self.model_1(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            QXA_2 = self.model_2(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss_1 = self.criterion(QXA_1, update.unsqueeze(1))\n",
    "            loss_2 = self.criterion(QXA_2, update.unsqueeze(1))\n",
    "            loss_1.backward()\n",
    "            loss_2.backward()\n",
    "            self.optimizer_1.step()\n",
    "            self.optimizer_2.step() \n",
    "\n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "\n",
    "        for _s in range(self.warmup_steps):\n",
    "            action = self.act(state, use_random=True)\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.memory.append(state, action, reward, next_state, trunc)\n",
    "        print(f\"size of warmup buffer is: {len(self.memory)}\")\n",
    "\n",
    "        while episode < max_episode:\n",
    "\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "\n",
    "\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = self.act(state, use_random=True)\n",
    "            else:\n",
    "\n",
    "                action = self.act(state)\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "\n",
    "            self.memory.append(state, action, reward, next_state, trunc)\n",
    "            episode_cum_reward += reward\n",
    "\n",
    "            # train\n",
    "            self.gradient_step()\n",
    "            # if self.update_target_strategy == 'replace':\n",
    "            #     if step % self.update_target_freq == 0: \n",
    "            #         self.target_model.load_state_dict(self.model.state_dict())\n",
    "            # if self.update_target_strategy == 'ema':\n",
    "            #     target_state_dict = self.target_model.state_dict()\n",
    "            #     model_state_dict = self.model.state_dict()\n",
    "            #     tau = self.update_target_tau\n",
    "            #     for key in model_state_dict:\n",
    "            #         target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "            #     self.target_model.load_state_dict(target_state_dict)\n",
    "\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if done or trunc:\n",
    "                episode += 1\n",
    "                print(\"Episode \", '{:3d}'.format(episode), \n",
    "                      \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                      \", buffer length \", '{:5d}'.format(len(self.memory)), \n",
    "                      #\", episode return \", '{:4.1f}'.format(episode_cum_reward),\n",
    "                      \", episode return \", '{:e}'.format(episode_cum_reward),\n",
    "                      sep='')\n",
    "                state, _ = env.reset()\n",
    "                episode_return.append(episode_cum_reward)\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        return episode_return\n",
    "\n",
    "    def act(self, observation, use_random=False):\n",
    "        if use_random:\n",
    "            return env.action_space.sample()\n",
    "        return torch.argmax(self.model_1(torch.FloatTensor(observation)).unsqueeze(0)).item()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model_1.state_dict(), \"best1.pt\")\n",
    "        torch.save(self.model_2.state_dict(), \"best2.pt\")\n",
    "    def load(self):\n",
    "        self.model_1.load_state_dict(torch.load(\"best1.pt\"))\n",
    "        self.model_2.load_state_dict(torch.load(\"best2.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'nb_actions': env.action_space.n,\n",
    "          'learning_rate': 0.0001,\n",
    "          'gamma': 0.98,\n",
    "          'buffer_size': 1e6,\n",
    "          'epsilon_min': 0.05,\n",
    "          'epsilon_max': 0.5,\n",
    "          'epsilon_decay_period': 10000,\n",
    "          'epsilon_delay_decay': 600,\n",
    "          #'update_target_strategy': 'ema',\n",
    "          #'update_target_freq': 50,\n",
    "          #'update_target_tau': 0.005,\n",
    "          'batch_size': 1024,\n",
    "          'warmup_steps': 2000}\n",
    "\n",
    "\n",
    "agent = ProjectAgent(config, DQN1, DQN2)\n",
    "scores = agent.train(env, 200)\n",
    "plt.plot(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
